https://pypi.org/project/easyocr/1.1.4/
https://www.jaided.ai/easyocr/tutorial/
tesseract doesnt work directly on files so
***** https://medium.com/@dr.booma19/extracting-text-from-pdf-files-using-ocr-a-step-by-step-guide-with-python-code-becf221529ef

focus on pdf files

https://medium.com/analytics-vidhya/question-answering-system-with-bert-ebe1130f8def

Token embeddings: A [CLS] token is added to the input word tokens at the beginning of the question and a [SEP] token is inserted at the end of both the question and the paragraph.
Segment embeddings: A marker indicating Sentence A or Sentence B is added to each token. This allows the model to distinguish between sentences. In the below example, all tokens marked as A belong to the question, and those marked as B belong to the paragraph.



https://huggingface.co/docs/transformers/en/model_doc/longformer  ---> longformer for lengthy text instead of BERT




There are different QA variants based on the inputs and outputs:

Extractive QA: The model extracts the answer from a context. The context here could be a provided text, a table or even HTML! This is usually solved with BERT-like models.
Open Generative QA: The model generates free text directly based on the context. You can learn more about the Text Generation task in its page.
Closed Generative QA: In this case, no context is provided. The answer is completely generated by a model.


Use Longformer for Contextual Encoding:
Longformer is designed for processing longer contexts. It can encode long text efficiently by attending locally and using global attention for specific tokens.
Use a Generative Model for Answer Generation:
After obtaining the encoded representation from Longformer, pass it to a generative model (like T5 or GPT) to generate the final answer




https://huggingface.co/docs/transformers/en/model_doc/gpt2#transformers.GPT2ForQuestionAnswering


https://stackoverflow.com/questions/52946920/bool-value-of-tensor-with-more-than-one-value-is-ambiguous-in-pytorch